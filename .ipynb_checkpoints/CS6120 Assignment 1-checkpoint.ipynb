{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Document Classification\n",
    "\n",
    "We must compute several values, \n",
    "\n",
    "Priors:\n",
    "\n",
    "$P(c)=\\frac{N_c}{N}$\n",
    "where $N_c$ is just number of documents with class and $N$ number of documents\n",
    "\n",
    "We will calculate the conditional probabilities of each word in the document. For the purposes of this calculation we will not calculate conditional probabilities for every single word, but only the words in D1 and D2\n",
    "\n",
    "Using $$P(w|c)=\\frac{count(w,c)+\\lambda}{count(c)+|V|\\cdot \\lambda}$$\n",
    "Using $\\lambda = 0.1$\n",
    "Example calculation:\n",
    "\n",
    "\n",
    "$P(rose|vegetable)=\\frac{0+0.1}{8+7\\cdot 0.1}$\n",
    "Other calculations outlined below\n",
    "\n",
    "We then find the maximum probablity of a document being in a class by using\n",
    "Where $c$ is class and $d$ document\n",
    "$P(c|d)=P(c) \\cdot \\prod_i^n{P(d_i|c)}$\n",
    "\n",
    "Example calculation:\n",
    "$P(flower|D1)=P(flower) \\cdot P(rose|flower) \\cdot P(lily|flower) \\cdot P(apple|flower) \\cdot P(carrot|flower)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1_flower 7.985671244629444e-07\n",
      "D1_fruit 2.490268929586247e-05\n",
      "D1_vegetable 5.732867232465228e-08\n"
     ]
    }
   ],
   "source": [
    "def p(wc, c, v, l=0.1):\n",
    "    return (wc + l)/(c + v * l)\n",
    "\n",
    "P={}\n",
    "\n",
    "P[('rose', 'vegetable')] = p(0, 8, 7)\n",
    "P[('lily', 'vegetable')] = p(0, 8, 2)\n",
    "P[('apple', 'vegetable')] = p(0, 8, 2)\n",
    "P[('carrot', 'vegetable')] = p(1, 8, 2)\n",
    "\n",
    "P[('rose', 'flower')] = p(6, 13, 7)\n",
    "P[('lily', 'flower')] = p(1, 13, 2)\n",
    "P[('apple', 'flower')] = p(0, 13, 2)\n",
    "P[('carrot', 'flower')] = p(0, 13, 2)\n",
    "\n",
    "P[('rose', 'fruit')] = p(1, 14, 7)\n",
    "P[('lily', 'fruit')] = p(1, 14, 2)\n",
    "P[('apple', 'fruit')] = p(2, 14, 2)\n",
    "P[('carrot', 'fruit')] = p(1, 14, 2)\n",
    "\n",
    "#Priors\n",
    "P['vegetable'] = 1/4\n",
    "P['flower'] = 3/8\n",
    "P['fruit'] = 3/8\n",
    "\n",
    "D1_flower = P['flower']*P[('rose', 'flower')]*P[('lily', 'flower')]*P[('apple', 'flower')]*P[('carrot', 'flower')]\n",
    "print(\"D1_flower\", D1_flower)\n",
    "D1_fruit = P['fruit']*P[('rose', 'fruit')]*P[('lily', 'fruit')]*P[('apple', 'fruit')]*P[('carrot', 'fruit')]\n",
    "print(\"D1_fruit\", D1_fruit)\n",
    "D1_vegetable = P['vegetable']*P[('rose', 'vegetable')]*P[('lily', 'vegetable')]*P[('apple', 'vegetable')]*P[('carrot', 'vegetable')]\n",
    "print(\"D1_vegetable\", D1_vegetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the argmax of these values and find that the fruit class is the most probable.\n",
    "\n",
    "Similarly for D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2_flower 1.47219552641001e-07\n",
      "D2_fruit 2.1008472857159783e-07\n",
      "D2_vegetable 2.618107011591733e-05\n"
     ]
    }
   ],
   "source": [
    "P[('pea', 'vegetable')] = p(2, 8, 3)\n",
    "P[('lotus', 'vegetable')] = p(1, 8, 2)\n",
    "P[('grape', 'vegetable')] = p(0, 8, 2)\n",
    "\n",
    "P[('pea', 'flower')] = p(1, 13, 3)\n",
    "P[('lotus', 'flower')] = p(0, 13, 2)\n",
    "P[('grape', 'flower')] = p(0, 13, 2)\n",
    "\n",
    "P[('pea', 'fruit')] = p(0, 14, 3)\n",
    "P[('lotus', 'fruit')] = p(1, 14, 2)\n",
    "P[('grape', 'fruit')] = p(2, 14, 2)\n",
    "\n",
    "D2_flower = P['flower']*(P[('pea', 'flower')]**2)*P[('lotus', 'flower')]*P[('grape', 'flower')]\n",
    "print(\"D2_flower\", D2_flower)\n",
    "D2_fruit = P['fruit']*(P[('pea', 'fruit')]**2)*P[('lotus', 'fruit')]*P[('grape', 'fruit')]\n",
    "print(\"D2_fruit\", D2_fruit)\n",
    "D2_vegetable = P['vegetable']*(P[('pea', 'vegetable')]**2)*P[('lotus', 'vegetable')]*P[('grape', 'vegetable')]\n",
    "print(\"D2_vegetable\", D2_vegetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that D2 is classed as vegetable\n",
    "\n",
    "# Word Sense Disambiguation\n",
    "\n",
    "Counting all the senses will be done by putting each word through wordnet\n",
    "\n",
    "In the cold weather, they started to the city. They were least worried protecting themselves\n",
    "against the common cold. After she signed the agreement, a cold chill crept up her spine.\n",
    "“Chill, its not that serious,” her husband assured and left to deposit cash at the bank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cdilg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cdilg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "\n",
    "raw = \"In the cold weather, they started to the city. They were least worried protecting themselves against the common cold. After she signed the agreement, a cold chill crept up her spine. Chill, its not that serious, her husband assured and left to deposit cash at the bank\"\n",
    "sents = [s.translate(str.maketrans('','',string.punctuation)).lower() for s in raw.strip().split(\".\")]\n",
    "\n",
    "sentence_senses = []\n",
    "word_senses = {}\n",
    "for s in sents:\n",
    "    sentencecount = 0\n",
    "    for word in s.split(' '):\n",
    "        syns = max([len(wn.synsets(word)), 1])\n",
    "        print(syns)\n",
    "        sentencecount *= syns\n",
    "        word_senses[word] = syns\n",
    "    sentence_senses += [sentencecount]\n",
    "    \n",
    "print(\"Total senses: \", np.product(np.array(sentence_senses)))\n",
    "print(\"Distinct combinations of senses: \", sentence_senses)\n",
    "#print(word_senses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modelling\n",
    "\n",
    "Implement a 4 gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "unk_threshold = 5\n",
    "cachefile = 'corpus.txt'\n",
    "unkfile = 'unk-corpus.txt'\n",
    "\n",
    "def save(corpus, file = cachefile):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(corpus, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read(file = cachefile):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "try:\n",
    "    text = read()\n",
    "except(FileNotFoundError):\n",
    "    corpus = \"\"\n",
    "    base = 'gutenberg'\n",
    "    for file in listdir(base):\n",
    "        for line in open(base + \"/\" + file):\n",
    "            corpus += ' ' + line.strip().lower().replace('  ', ' ')\n",
    "    text = word_tokenize(corpus) \n",
    "    save(text)\n",
    "\n",
    "wordcount = Counter(text)\n",
    "#print(wordcount)\n",
    "\n",
    "# we need to remove words that occur less than 5 times and replace with UNK\n",
    "# count the items in the list. Figure out which ones are greater \n",
    "# unkwords = [w for w in [w for w in wordcount.keys() if wordcount[w] <= unk_threshold]]\n",
    "\n",
    "# we want a list of indices for which to replace with 'UNK'\n",
    "# go through the list, keep an index of where each word ocurrs. \n",
    "# at the end, count all of the lengths of these lists\n",
    "# for each list which is less than 5. go to the text list and replace each element with 'UNK'\n",
    "\n",
    "def replace_unk(text, threshold):\n",
    "    try:\n",
    "        return read(unkfile)\n",
    "    except(FileNotFoundError):\n",
    "        counterdict = {}\n",
    "        for i, t in enumerate(text):\n",
    "            if t in counterdict.keys():\n",
    "                counterdict[t].append(i)\n",
    "            else:\n",
    "                counterdict[t] = [i]\n",
    "\n",
    "        for locations in counterdict:\n",
    "            #print(locations, len(counterdict[locations]))\n",
    "            if len(counterdict[locations]) <= threshold:\n",
    "                for loc in counterdict[locations]:\n",
    "                    text[loc] = 'UNK'\n",
    "        save(text, unkfile)\n",
    "        return text\n",
    "text = replace_unk(text, unk_threshold)\n",
    "\n",
    "#find out the definition of 4 gram counts\n",
    "#probably count all of the ways 3 previous words occur\n",
    "#make a big table\n",
    "\n",
    "def ngram(n, text):\n",
    "    ngrams = {}\n",
    "    for i in range(n, len(text)+1):\n",
    "        #get the previous n words.\n",
    "        gram = tuple(text[i-n:i])\n",
    "        if gram in ngrams.keys():\n",
    "            ngrams[gram] += 1\n",
    "        else:\n",
    "             ngrams[gram] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    with open('ngrams.txt', 'w') as f:\n",
    "        f.write(' '.join(\"{} {}\".format(key,val) for (key,val) in ngrams.items()))\n",
    "    return ngrams\n",
    "\n",
    "ngram(4, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

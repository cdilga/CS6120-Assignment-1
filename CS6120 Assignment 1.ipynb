{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Document Classification\n",
    "\n",
    "We must compute several values, \n",
    "\n",
    "Priors:\n",
    "\n",
    "$P(c)=\\frac{N_c}{N}$\n",
    "where $N_c$ is just number of documents with class and $N$ number of documents\n",
    "\n",
    "We will calculate the conditional probabilities of each word in the document. For the purposes of this calculation we will not calculate conditional probabilities for every single word, but only the words in D1 and D2\n",
    "\n",
    "Using $$P(w|c)=\\frac{count(w,c)+\\lambda}{count(c)+|V|\\cdot \\lambda}$$\n",
    "Using $\\lambda = 0.1$\n",
    "Example calculation:\n",
    "\n",
    "\n",
    "$P(rose|vegetable)=\\frac{0+0.1}{8+7\\cdot 0.1}$\n",
    "Other calculations outlined below\n",
    "\n",
    "We then find the maximum probablity of a document being in a class by using\n",
    "Where $c$ is class and $d$ document\n",
    "$P(c|d)=P(c) \\cdot \\prod_i^n{P(d_i|c)}$\n",
    "\n",
    "Example calculation:\n",
    "$P(flower|D1)=P(flower) \\cdot P(rose|flower) \\cdot P(lily|flower) \\cdot P(apple|flower) \\cdot P(carrot|flower)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1_flower 7.985671244629444e-07\n",
      "D1_fruit 2.490268929586247e-05\n",
      "D1_vegetable 5.732867232465228e-08\n"
     ]
    }
   ],
   "source": [
    "def p(wc, c, v, l=0.1):\n",
    "    return (wc + l)/(c + v * l)\n",
    "\n",
    "P={}\n",
    "\n",
    "P[('rose', 'vegetable')] = p(0, 8, 7)\n",
    "P[('lily', 'vegetable')] = p(0, 8, 2)\n",
    "P[('apple', 'vegetable')] = p(0, 8, 2)\n",
    "P[('carrot', 'vegetable')] = p(1, 8, 2)\n",
    "\n",
    "P[('rose', 'flower')] = p(6, 13, 7)\n",
    "P[('lily', 'flower')] = p(1, 13, 2)\n",
    "P[('apple', 'flower')] = p(0, 13, 2)\n",
    "P[('carrot', 'flower')] = p(0, 13, 2)\n",
    "\n",
    "P[('rose', 'fruit')] = p(1, 14, 7)\n",
    "P[('lily', 'fruit')] = p(1, 14, 2)\n",
    "P[('apple', 'fruit')] = p(2, 14, 2)\n",
    "P[('carrot', 'fruit')] = p(1, 14, 2)\n",
    "\n",
    "#Priors\n",
    "P['vegetable'] = 1/4\n",
    "P['flower'] = 3/8\n",
    "P['fruit'] = 3/8\n",
    "\n",
    "D1_flower = P['flower']*P[('rose', 'flower')]*P[('lily', 'flower')]*P[('apple', 'flower')]*P[('carrot', 'flower')]\n",
    "print(\"D1_flower\", D1_flower)\n",
    "D1_fruit = P['fruit']*P[('rose', 'fruit')]*P[('lily', 'fruit')]*P[('apple', 'fruit')]*P[('carrot', 'fruit')]\n",
    "print(\"D1_fruit\", D1_fruit)\n",
    "D1_vegetable = P['vegetable']*P[('rose', 'vegetable')]*P[('lily', 'vegetable')]*P[('apple', 'vegetable')]*P[('carrot', 'vegetable')]\n",
    "print(\"D1_vegetable\", D1_vegetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the argmax of these values and find that the fruit class is the most probable.\n",
    "\n",
    "Similarly for D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2_flower 1.47219552641001e-07\n",
      "D2_fruit 2.1008472857159783e-07\n",
      "D2_vegetable 2.618107011591733e-05\n"
     ]
    }
   ],
   "source": [
    "P[('pea', 'vegetable')] = p(2, 8, 3)\n",
    "P[('lotus', 'vegetable')] = p(1, 8, 2)\n",
    "P[('grape', 'vegetable')] = p(0, 8, 2)\n",
    "\n",
    "P[('pea', 'flower')] = p(1, 13, 3)\n",
    "P[('lotus', 'flower')] = p(0, 13, 2)\n",
    "P[('grape', 'flower')] = p(0, 13, 2)\n",
    "\n",
    "P[('pea', 'fruit')] = p(0, 14, 3)\n",
    "P[('lotus', 'fruit')] = p(1, 14, 2)\n",
    "P[('grape', 'fruit')] = p(2, 14, 2)\n",
    "\n",
    "D2_flower = P['flower']*(P[('pea', 'flower')]**2)*P[('lotus', 'flower')]*P[('grape', 'flower')]\n",
    "print(\"D2_flower\", D2_flower)\n",
    "D2_fruit = P['fruit']*(P[('pea', 'fruit')]**2)*P[('lotus', 'fruit')]*P[('grape', 'fruit')]\n",
    "print(\"D2_fruit\", D2_fruit)\n",
    "D2_vegetable = P['vegetable']*(P[('pea', 'vegetable')]**2)*P[('lotus', 'vegetable')]*P[('grape', 'vegetable')]\n",
    "print(\"D2_vegetable\", D2_vegetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that D2 is classed as vegetable\n",
    "\n",
    "# Word Sense Disambiguation\n",
    "\n",
    "Counting all the senses will be done by putting each word through wordnet\n",
    "\n",
    "In the cold weather, they started to the city. They were least worried protecting themselves\n",
    "against the common cold. After she signed the agreement, a cold chill crept up her spine.\n",
    "“Chill, its not that serious,” her husband assured and left to deposit cash at the bank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cdilg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cdilg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total senses:  0\n",
      "Distinct combinations of senses:  [0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "raw = \"In the cold weather, they started to the city. They were least worried protecting themselves against the common cold. After she signed the agreement, a cold chill crept up her spine. Chill, its not that serious, her husband assured and left to deposit cash at the bank\"\n",
    "sents = [s.translate(str.maketrans('','', string.punctuation)).lower() for s in raw.strip().split(\".\")]\n",
    "\n",
    "sentence_senses = []\n",
    "word_senses = {}\n",
    "for s in sents:\n",
    "    sentencecount = 0\n",
    "    for word in s.split(' '):\n",
    "        syns = max([len(wn.synsets(word)), 1])\n",
    "        sentencecount *= syns\n",
    "        word_senses[word] = syns\n",
    "    sentence_senses += [sentencecount]\n",
    "    \n",
    "print(\"Total senses: \", np.product(np.array(sentence_senses)))\n",
    "print(\"Distinct combinations of senses: \", sentence_senses)\n",
    "#print(word_senses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is not working\n",
    "# please investigate\n",
    "# seems to be something to do with how the lists are being accessed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modelling\n",
    "\n",
    "Implement a 4 gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def save(corpus, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(corpus, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def read_dir(directory, cachefile):\n",
    "    try:\n",
    "        text = read(cachefile)\n",
    "    except(FileNotFoundError):\n",
    "        corpus = \"\"\n",
    "        base = directory\n",
    "        for file in listdir(base):\n",
    "            for line in open(base + \"/\" + file):\n",
    "                corpus += ' ' + line.strip().lower().replace('  ', ' ')\n",
    "        text = word_tokenize(corpus) \n",
    "        save(text, cachefile)\n",
    "    finally:\n",
    "        return text\n",
    "\n",
    "# we need to remove words that occur less than 5 times and replace with UNK\n",
    "# count the items in the list. Figure out which ones are greater \n",
    "# unkwords = [w for w in [w for w in wordcount.keys() if wordcount[w] <= unk_threshold]]\n",
    "\n",
    "# we want a list of indices for which to replace with 'UNK'\n",
    "# go through the list, keep an index of where each word ocurrs. \n",
    "# at the end, count all of the lengths of these lists\n",
    "# for each list which is less than 5. go to the text list and replace each element with 'UNK'\n",
    "\n",
    "def replace_unk(text, threshold, savefile):\n",
    "    try:\n",
    "        return read(savefile)\n",
    "    except(FileNotFoundError):\n",
    "        counterdict = {}\n",
    "        for i, t in enumerate(text):\n",
    "            if t in counterdict.keys():\n",
    "                counterdict[t].append(i)\n",
    "            else:\n",
    "                counterdict[t] = [i]\n",
    "\n",
    "        for locations in counterdict:\n",
    "            #print(locations, len(counterdict[locations]))\n",
    "            if len(counterdict[locations]) <= threshold:\n",
    "                for loc in counterdict[locations]:\n",
    "                    text[loc] = 'UNK'\n",
    "        save(text, savefile)\n",
    "        return text\n",
    "\n",
    "#find out the definition of 4 gram counts\n",
    "#probably count all of the ways 3 previous words occur\n",
    "#make a big table\n",
    "\n",
    "def ngram(n, text, outfile):\n",
    "    ngrams = {}\n",
    "    for i in range(n, len(text)+1):\n",
    "        #get the previous n words.\n",
    "        gram = tuple(text[i-n:i])\n",
    "        if gram in ngrams.keys():\n",
    "            ngrams[gram] += 1\n",
    "        else:\n",
    "             ngrams[gram] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        for line in sorted(ngrams, key=ngrams.get, reverse=True):\n",
    "            f.write(' '.join(line) + ' ' + str(ngrams[line]) + '\\n')\n",
    "    return ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_dir('gutenberg', 'gutenberg-corpus.txt')\n",
    "text = replace_unk(text, 5, 'gutenberg-unk.txt')\n",
    "guten4 = ngram(4, text, 'gutenberg-4grams.txt')\n",
    "guten3 = ngram(3, text, 'gutenberg-3grams.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity formula: $PP(W)=\\left(\\prod_{i=1}^N{\\frac{1}{P(w|w_{n-1}, w_{n-2}, w_{n-3}, w_{n-4})}}\\right)^\\frac{1}{N}$\n",
    "\n",
    "\n",
    "Probability of words: $P(w)=\\frac{num\\space counts}{total \\space words}$\n",
    "$P(w_n|w_{n-1}, w_{n-2}, w_{n-3}, w_{n-4})=\\frac{C(w_{n-4}w_{n-3}w_{n-2}w_{n-1}w)+0.1}{C(w_{n-4}w_{n-3}w_{n-2}w_{n-1})+|V|\\times 0.1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030798517076259754\n",
      "7.797958291094393e-05\n",
      "1.0000001605774118\n",
      "1.000070634697219\n"
     ]
    }
   ],
   "source": [
    "imdb = read_dir('imdb_data', 'imdb-corpus.txt')\n",
    "imdb = replace_unk(imdb, 5, 'imdb-unk.txt')\n",
    "#imdbmodel = ngram(4, imdb, 'imdb-ngrams.txt')\n",
    "#imdbmodel3 = ngram(3, imdb, 'imdb-3grams.txt')\n",
    "import math\n",
    "\n",
    "def calculate_probability(model, onelessmodel, word, context, n = 4, lam = 0.1):\n",
    "    #for each of the words, we need the prior 4 words. We will look this up and find whether we have a match\n",
    "    try:\n",
    "        ret = (model[(context[0], context[1], context[2], word)] + lam)/(onelessmodel[(context[0], context[1], context[2])]+len(model)*lam)\n",
    "    except:\n",
    "        ret = lam/(len(model)*lam)\n",
    "    #here v is a vocabulary of n-grams, so will be the count of the ngrams\n",
    "    \n",
    "    return ret\n",
    "\n",
    "        \n",
    "def perplexity(model, onelessmodel, text, n = 4):\n",
    "    #this takes in text, which the existing probabilities and counts are used to \n",
    "    #come up with a number, all of the probabilities multiplied together. We probably can use log for this?\n",
    "    #Perplexity is a measure of how probable the model is at generating a sentence\n",
    "    #\n",
    "    #Perplexity is an integer - lower better\n",
    "    pp = 1\n",
    "    V = len(model)\n",
    "    for i, word in enumerate(text):\n",
    "        #calculate the probability of this word\n",
    "        #TODO implement log sum instead\n",
    "        pp *= 1/calculate_probability(model, onelessmodel, word, text[i-n:i-1])\n",
    "    return math.pow(pp, 1/V)\n",
    "\n",
    "def wikiperplexity(model, onelessmodel, text, n=4):\n",
    "    pp = 0\n",
    "    V = len(model)\n",
    "    for i, word in enumerate(text):\n",
    "        pp -= math.log(2, calculate_probability(model, onelessmodel, word, text[i-n:i-1]))\n",
    "    return math.pow(2, pp/V)\n",
    "#Currently, this works: \n",
    "#print(guten4[('the', 'children', 'of', 'israel')])\n",
    "\n",
    "print(calculate_probability(guten4, guten3, 'israel', ['the', 'children', 'of']))\n",
    "print(calculate_probability(guten4, guten3, 'were', ['children', 'of', 'israel']))\n",
    "print(wikiperplexity(guten4, guten3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))\n",
    "print(perplexity(guten4, guten3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))\n",
    "#perplexity(guten4, guten3, [\"the\", \"children\", \"of\", \"israel\", \"are\", \"well\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000033056111426\n",
      "1.0009467110114756\n"
     ]
    }
   ],
   "source": [
    "news = read_dir('news_data', 'news-corpus.txt')\n",
    "news = replace_unk(news, 5, 'news-unk.txt')\n",
    "news4 = ngram(4, news, 'news-4grams.txt')\n",
    "news3 = ngram(3, news, 'news-3grams.txt')\n",
    "\n",
    "print(wikiperplexity(news4, news3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))\n",
    "print(perplexity(news4, news3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging HMM\n",
    "\n",
    "First find the tag unigram and tag bigram counts from the corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('np', '<s>')\n",
      "('Mickey', 'np')\n",
      "(',', 'np')\n",
      "(',', ',')\n",
      "('bez*', ',')\n",
      "(\"isn't\", 'bez*')\n",
      "('jj', 'bez*')\n",
      "('bad', 'jj')\n",
      "('jj', 'jj')\n",
      "('high', 'jj')\n",
      "('nn', 'jj')\n",
      "('loot', 'nn')\n",
      "('in', 'nn')\n",
      "('regarding', 'in')\n",
      "('cd', 'in')\n",
      "('6-12', 'cd')\n",
      "('nns', 'cd')\n",
      "('preparations', 'nns')\n",
      "('cc', 'nns')\n",
      "('and', 'cc')\n",
      "('at', 'cc')\n",
      "('the', 'at')\n",
      "('jj', 'at')\n",
      "('old', 'jj')\n",
      "('nns', 'jj')\n",
      "('doors', 'nns')\n",
      "(\"''\", 'nns')\n",
      "(\"''\", \"''\")\n",
      "(',', \"''\")\n",
      "(',', ',')\n",
      "('cc', ',')\n",
      "('or', 'cc')\n",
      "('jj', 'cc')\n",
      "('anionic', 'jj')\n",
      "('--', 'jj')\n",
      "('--', '--')\n",
      "('in', '--')\n",
      "('in', 'in')\n",
      "('ppo', 'in')\n",
      "('her', 'ppo')\n",
      "('.', 'ppo')\n",
      "('.', '.')\n",
      "('.', '.')\n",
      "('!', '.')\n",
      "('.', '.')\n",
      "('!', '.')\n",
      "('.', '.')\n",
      "('.', '.')\n",
      "('.', '.')\n",
      "('.', '.')\n",
      "Mickey/np ,/, isn't/bez* bad/jj high/jj loot/nn regarding/in 6-12/cd preparations/nns and/cc the/at old/jj doors/nns ''/'' ,/, or/cc anionic/jj --/-- in/in her/ppo ./. !/. !/. ./. ./.\n",
      "Mickey , isn't bad high loot regarding 6-12 preparations and the old doors '' , or anionic -- in her . ! ! . .\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import random\n",
    "#read in the file/s?\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#nltk.download('brown')\n",
    "def read_brown(directory, cachefile):\n",
    "    try:\n",
    "        text = read(cachefile)\n",
    "    except(FileNotFoundError):\n",
    "        corpus = []\n",
    "        base = directory\n",
    "        for file in listdir(base):\n",
    "            #print(file)\n",
    "            for sent in open(base + \"/\" + file):\n",
    "                if sent == \"\\n\": continue\n",
    "                wordlist = []\n",
    "                for word in sent.strip().split(' '):\n",
    "                    #split the word and it's tag\n",
    "                    if word == '': \n",
    "                        continue\n",
    "                    wordlist.append(word.split('/'))\n",
    "                    \n",
    "                corpus += [['<s>', '<s>']] + wordlist + [['</s>', '</s>']]\n",
    "        text = corpus\n",
    "        save(text, cachefile)\n",
    "    finally:\n",
    "        return text\n",
    "brown = read_brown('brown', 'brown-cache.txt')\n",
    "\n",
    "#calculate the word-tag counts\n",
    "#lets do this in the same dictionary way we did earlier\n",
    "\n",
    "def wordtag(text, outfile):\n",
    "    pairs = {}\n",
    "    for word in text:\n",
    "        #ignore the sentence tags\n",
    "        if (word == ['<s>', '<s>'] or word == ['</s>', '</s>']): continue\n",
    "        tagpair = tuple(word)\n",
    "        if tagpair in pairs.keys():\n",
    "            pairs[tagpair] += 1\n",
    "        else:\n",
    "             pairs[tagpair] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        for word in sorted(pairs, key=pairs.get, reverse=True):\n",
    "            f.write(' '.join(word) + ' ' + str(pairs[word]) + '\\n')\n",
    "    return pairs\n",
    "\n",
    "def tagunigram(text, outfile):\n",
    "    '''This is literally a unigram of the tag, t_n. That is we \n",
    "    will not consider the word association and will instead just\n",
    "    consider the impact of the counts of tags themselves.'''\n",
    "    unigrams = {}\n",
    "    for word in text:\n",
    "        #ignore the sentence tags\n",
    "        if (word == ['<s>', '<s>'] or word == ['</s>', '</s>']): continue \n",
    "        tag = word[1]\n",
    "        if tag in unigrams.keys():\n",
    "            unigrams[tag] += 1\n",
    "        else:\n",
    "             unigrams[tag] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        #TODO there is a problem with the way this joins - it's assuming a tuple\n",
    "        for word in sorted(unigrams, key=unigrams.get, reverse=True):\n",
    "            f.write(' '.join(word) + ' ' + str(unigrams[word]) + '\\n')\n",
    "    return unigrams\n",
    "    \n",
    "def savecounts(d, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for token in sorted(d, key=d.get, reverse=True):\n",
    "            f.write(' '.join(token) + ' ' + str(d[token]) + '\\n')\n",
    "            \n",
    "def tagbigram(text, outfile):\n",
    "    '''Here we consider both t_n and t_n-1 and report the counts. \n",
    "    Again we do not stop to consider the effects of the word association'''\n",
    "    bigrams = {}\n",
    "    for i in range(len(text)):\n",
    "        #ignore the sentence tags\n",
    "        if (text[i] == ['<s>', '<s>'] or text[i] == ['</s>', '</s>']): continue\n",
    "        t = text[i][1]\n",
    "        t1 = text[i-1][1]\n",
    "        if (t, t1) in bigrams.keys():\n",
    "            bigrams[(t, t1)] += 1\n",
    "        else:\n",
    "             bigrams[(t, t1)] = 1\n",
    "    savecounts(bigrams, outfile)\n",
    "    return bigrams\n",
    "    #save a textual representation of the dict to file\n",
    "\n",
    "\n",
    "def transition(bigramtags, unigramtags):\n",
    "    probabilities = {}\n",
    "    for bigram in bigramtags.keys():\n",
    "        probabilities[bigram] = bigramtags[bigram]/unigramtags[bigram[0]]\n",
    "    savecounts(probabilities, 'brownmeta/transition-probabilities.txt')\n",
    "    return probabilities\n",
    "\n",
    "def emission(wordtags, unigramtags):\n",
    "    emissionprob = {}\n",
    "    for wordpair in wordtags.keys():\n",
    "        emissionprob[wordpair] = wordtags[wordpair]/unigramtags[wordpair[1]]\n",
    "    savecounts(emissionprob, 'brownmeta/emission-probabilities.txt')\n",
    "    return emissionprob\n",
    "\n",
    "class postagger:\n",
    "    \n",
    "    def __init__(self, wordtags, unigramprobabilities, bigramprobabilities):\n",
    "        self.wt = wordtags\n",
    "        self.up = unigramprobabilities\n",
    "        self.bp = bigramprobabilities\n",
    "    \n",
    "    def nextword(self, dct):\n",
    "        #select a next item based on a random number which is weighted by the probabilities\n",
    "        \n",
    "        #sum all of the probabilities and normalize\n",
    "        tot = 0\n",
    "        for value in dct.keys():\n",
    "            tot += dct[value]\n",
    "        normalised = {}\n",
    "        index = random.random()\n",
    "        for pair in dct.keys():\n",
    "            index -= dct[pair] / tot\n",
    "            if index <= 0.0:\n",
    "                return pair\n",
    "        \n",
    "    def predictSentence(self):\n",
    "        #lets just feed in a start tag, \n",
    "        #pick the highest probability next tag, and then \n",
    "        #the highest probability word of that tag\n",
    "        \n",
    "        #find all things with a start tag at the start\n",
    "        \n",
    "        sent = []\n",
    "        humansent = []\n",
    "        priortag = '<s>'\n",
    "        for i in range(25):\n",
    "            subset = {}\n",
    "            for tags in self.bp.keys():\n",
    "                if priortag == tags[1]:\n",
    "                    subset[tags] = self.bp[tags]\n",
    "            selectedbigram = self.nextword(subset)\n",
    "            \n",
    "            print(selectedbigram)\n",
    "            \n",
    "            currenttag = selectedbigram[0]\n",
    "            \n",
    "            potentialwordtags = {}\n",
    "            \n",
    "            for wordtag in self.wt.keys():\n",
    "                if currenttag == wordtag[1]:\n",
    "                    potentialwordtags[wordtag] = self.wt[wordtag]\n",
    "            currentword = self.nextword(potentialwordtags)\n",
    "            \n",
    "            print(currentword)\n",
    "            \n",
    "            sent.append('/'.join(currentword))\n",
    "            humansent.append(currentword[0])\n",
    "            priortag = currenttag\n",
    "        print(' '.join(sent))\n",
    "        print(' '.join(humansent))\n",
    "    \n",
    "    \n",
    "tags = wordtag(brown, 'brownmeta/brownwordtag.txt')\n",
    "unitags = tagunigram(brown, 'brownmeta/brownuni.txt')\n",
    "bitags = tagbigram(brown, 'brownmeta/brownbigrams.txt')\n",
    "\n",
    "transitionprobs = transition(bitags, unitags)\n",
    "emissionprobs = emission(tags, unitags)\n",
    "\n",
    "pos = postagger(tags, unitags, bitags)\n",
    "#for i in range(1000):\n",
    "#    print(pos.nextword({('flog', 'np'): 0.5, ('dog', 'nn'):0.9, ('whack', 'dd'):0.01}))\n",
    "pos.predictSentence()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Document Classification\n",
    "\n",
    "We must compute several values, \n",
    "\n",
    "Priors:\n",
    "\n",
    "$P(c)=\\frac{N_c}{N}$\n",
    "where $N_c$ is just number of documents with class and $N$ number of documents\n",
    "\n",
    "We will calculate the conditional probabilities of each word in the document. For the purposes of this calculation we will not calculate conditional probabilities for every single word, but only the words in D1 and D2\n",
    "\n",
    "Using $$P(w|c)=\\frac{count(w,c)+\\lambda}{count(c)+|V|\\cdot \\lambda}$$\n",
    "Using $\\lambda = 0.1$\n",
    "Example calculation:\n",
    "\n",
    "\n",
    "$P(rose|vegetable)=\\frac{0+0.1}{8+7\\cdot 0.1}$\n",
    "Other calculations outlined below\n",
    "\n",
    "We then find the maximum probablity of a document being in a class by using\n",
    "Where $c$ is class and $d$ document\n",
    "$P(c|d)=P(c) \\cdot \\prod_i^n{P(d_i|c)}$\n",
    "\n",
    "Example calculation:\n",
    "$P(flower|D1)=P(flower) \\cdot P(rose|flower) \\cdot P(lily|flower) \\cdot P(apple|flower) \\cdot P(carrot|flower)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D1_flower 7.985671244629444e-07\n",
      "D1_fruit 2.490268929586247e-05\n",
      "D1_vegetable 5.732867232465228e-08\n"
     ]
    }
   ],
   "source": [
    "def p(wc, c, v, l=0.1):\n",
    "    return (wc + l)/(c + v * l)\n",
    "\n",
    "P={}\n",
    "\n",
    "P[('rose', 'vegetable')] = p(0, 8, 7)\n",
    "P[('lily', 'vegetable')] = p(0, 8, 2)\n",
    "P[('apple', 'vegetable')] = p(0, 8, 2)\n",
    "P[('carrot', 'vegetable')] = p(1, 8, 2)\n",
    "\n",
    "P[('rose', 'flower')] = p(6, 13, 7)\n",
    "P[('lily', 'flower')] = p(1, 13, 2)\n",
    "P[('apple', 'flower')] = p(0, 13, 2)\n",
    "P[('carrot', 'flower')] = p(0, 13, 2)\n",
    "\n",
    "P[('rose', 'fruit')] = p(1, 14, 7)\n",
    "P[('lily', 'fruit')] = p(1, 14, 2)\n",
    "P[('apple', 'fruit')] = p(2, 14, 2)\n",
    "P[('carrot', 'fruit')] = p(1, 14, 2)\n",
    "\n",
    "#Priors\n",
    "P['vegetable'] = 1/4\n",
    "P['flower'] = 3/8\n",
    "P['fruit'] = 3/8\n",
    "\n",
    "D1_flower = P['flower']*P[('rose', 'flower')]*P[('lily', 'flower')]*P[('apple', 'flower')]*P[('carrot', 'flower')]\n",
    "print(\"D1_flower\", D1_flower)\n",
    "D1_fruit = P['fruit']*P[('rose', 'fruit')]*P[('lily', 'fruit')]*P[('apple', 'fruit')]*P[('carrot', 'fruit')]\n",
    "print(\"D1_fruit\", D1_fruit)\n",
    "D1_vegetable = P['vegetable']*P[('rose', 'vegetable')]*P[('lily', 'vegetable')]*P[('apple', 'vegetable')]*P[('carrot', 'vegetable')]\n",
    "print(\"D1_vegetable\", D1_vegetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the argmax of these values and find that the fruit class is the most probable.\n",
    "\n",
    "Similarly for D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2_flower 1.47219552641001e-07\n",
      "D2_fruit 2.1008472857159783e-07\n",
      "D2_vegetable 2.618107011591733e-05\n"
     ]
    }
   ],
   "source": [
    "P[('pea', 'vegetable')] = p(2, 8, 3)\n",
    "P[('lotus', 'vegetable')] = p(1, 8, 2)\n",
    "P[('grape', 'vegetable')] = p(0, 8, 2)\n",
    "\n",
    "P[('pea', 'flower')] = p(1, 13, 3)\n",
    "P[('lotus', 'flower')] = p(0, 13, 2)\n",
    "P[('grape', 'flower')] = p(0, 13, 2)\n",
    "\n",
    "P[('pea', 'fruit')] = p(0, 14, 3)\n",
    "P[('lotus', 'fruit')] = p(1, 14, 2)\n",
    "P[('grape', 'fruit')] = p(2, 14, 2)\n",
    "\n",
    "D2_flower = P['flower']*(P[('pea', 'flower')]**2)*P[('lotus', 'flower')]*P[('grape', 'flower')]\n",
    "print(\"D2_flower\", D2_flower)\n",
    "D2_fruit = P['fruit']*(P[('pea', 'fruit')]**2)*P[('lotus', 'fruit')]*P[('grape', 'fruit')]\n",
    "print(\"D2_fruit\", D2_fruit)\n",
    "D2_vegetable = P['vegetable']*(P[('pea', 'vegetable')]**2)*P[('lotus', 'vegetable')]*P[('grape', 'vegetable')]\n",
    "print(\"D2_vegetable\", D2_vegetable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that D2 is classed as vegetable\n",
    "\n",
    "# Word Sense Disambiguation\n",
    "\n",
    "Counting all the senses will be done by putting each word through wordnet\n",
    "\n",
    "In the cold weather, they started to the city. They were least worried protecting themselves\n",
    "against the common cold. After she signed the agreement, a cold chill crept up her spine.\n",
    "“Chill, its not that serious,” her husband assured and left to deposit cash at the bank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cdilg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\cdilg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "download('wordnet')\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total senses:  4654630885130005118976000\n",
      "Distinct combinations of senses per sentence:  [28224, 149760, 39513600, 27869184]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "raw = \"In the cold weather, they started to the city. They were least worried protecting themselves against the common cold. After she signed the agreement, a cold chill crept up her spine. Chill, its not that serious, her husband assured and left to deposit cash at the bank\"\n",
    "sents = [s.translate(str.maketrans('','', string.punctuation)).lower() for s in raw.strip().split(\".\")]\n",
    "\n",
    "sentence_senses = []\n",
    "word_senses = {}\n",
    "for s in sents:\n",
    "    sentencecount = 1\n",
    "    for word in s.split(' '):\n",
    "        syns = max([len(wn.synsets(word)), 1])\n",
    "        sentencecount *= syns\n",
    "        word_senses[word] = syns\n",
    "    sentence_senses += [sentencecount]\n",
    "\n",
    "total = 1\n",
    "for sentence in sentence_senses:\n",
    "    total *= sentence\n",
    "print(\"Total senses: \", total)\n",
    "print(\"Distinct combinations of senses per sentence: \", sentence_senses)\n",
    "#print(word_senses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see how many different ways all of the senses could be combined, broken down by sentence. These numbers are extremely large for a relatively short sentence of only around 10 words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modelling\n",
    "\n",
    "Implement a 4 gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def save(corpus, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(corpus, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def read_dir(directory, cachefile):\n",
    "    try:\n",
    "        text = read(cachefile)\n",
    "    except(FileNotFoundError):\n",
    "        corpus = \"\"\n",
    "        base = directory\n",
    "        for file in listdir(base):\n",
    "            for line in open(base + \"/\" + file):\n",
    "                corpus += ' ' + line.strip().lower().replace('  ', ' ')\n",
    "        text = word_tokenize(corpus) \n",
    "        save(text, cachefile)\n",
    "    finally:\n",
    "        return text\n",
    "\n",
    "# we need to remove words that occur less than 5 times and replace with UNK\n",
    "# count the items in the list. Figure out which ones are greater \n",
    "# unkwords = [w for w in [w for w in wordcount.keys() if wordcount[w] <= unk_threshold]]\n",
    "\n",
    "# we want a list of indices for which to replace with 'UNK'\n",
    "# go through the list, keep an index of where each word ocurrs. \n",
    "# at the end, count all of the lengths of these lists\n",
    "# for each list which is less than 5. go to the text list and replace each element with 'UNK'\n",
    "\n",
    "def replace_unk(text, threshold, savefile):\n",
    "    try:\n",
    "        return read(savefile)\n",
    "    except(FileNotFoundError):\n",
    "        counterdict = {}\n",
    "        for i, t in enumerate(text):\n",
    "            if t in counterdict.keys():\n",
    "                counterdict[t].append(i)\n",
    "            else:\n",
    "                counterdict[t] = [i]\n",
    "\n",
    "        for locations in counterdict:\n",
    "            #print(locations, len(counterdict[locations]))\n",
    "            if len(counterdict[locations]) <= threshold:\n",
    "                for loc in counterdict[locations]:\n",
    "                    text[loc] = 'UNK'\n",
    "        save(text, savefile)\n",
    "        return text\n",
    "\n",
    "#find out the definition of 4 gram counts\n",
    "#probably count all of the ways 3 previous words occur\n",
    "#make a big table\n",
    "\n",
    "def ngram(n, text, outfile):\n",
    "    ngrams = {}\n",
    "    for i in range(n, len(text)+1):\n",
    "        #get the previous n words.\n",
    "        gram = tuple(text[i-n:i])\n",
    "        if gram in ngrams.keys():\n",
    "            ngrams[gram] += 1\n",
    "        else:\n",
    "             ngrams[gram] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        for line in sorted(ngrams, key=ngrams.get, reverse=True):\n",
    "            f.write(' '.join(line) + ' ' + str(ngrams[line]) + '\\n')\n",
    "    return ngrams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_dir('gutenberg', 'gutenberg-corpus.txt')\n",
    "text = replace_unk(text, 5, 'gutenberg-unk.txt')\n",
    "guten4 = ngram(4, text, 'gutenberg-4grams.txt')\n",
    "guten3 = ngram(3, text, 'gutenberg-3grams.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity formula: $PP(W)=\\left(\\prod_{i=1}^N{\\frac{1}{P(w|w_{n-1}, w_{n-2}, w_{n-3}, w_{n-4})}}\\right)^\\frac{1}{N}$\n",
    "\n",
    "\n",
    "Probability of words: $P(w)=\\frac{num\\space counts}{total \\space words}$\n",
    "$P(w_n|w_{n-1}, w_{n-2}, w_{n-3}, w_{n-4})=\\frac{C(w_{n-4}w_{n-3}w_{n-2}w_{n-1}w)+0.1}{C(w_{n-4}w_{n-3}w_{n-2}w_{n-1})+|V|\\times 0.1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0030798517076259754\n",
      "7.797958291094393e-05\n",
      "1.0000001605774118\n",
      "1.000070634697219\n"
     ]
    }
   ],
   "source": [
    "imdb = read_dir('imdb_data', 'imdb-corpus.txt')\n",
    "imdb = replace_unk(imdb, 5, 'imdb-unk.txt')\n",
    "#imdbmodel = ngram(4, imdb, 'imdb-ngrams.txt')\n",
    "#imdbmodel3 = ngram(3, imdb, 'imdb-3grams.txt')\n",
    "import math\n",
    "\n",
    "def calculate_probability(model, onelessmodel, word, context, n = 4, lam = 0.1):\n",
    "    #for each of the words, we need the prior 4 words. We will look this up and find whether we have a match\n",
    "    try:\n",
    "        ret = (model[(context[0], context[1], context[2], word)] + lam)/(onelessmodel[(context[0], context[1], context[2])]+len(model)*lam)\n",
    "    except:\n",
    "        ret = lam/(len(model)*lam)\n",
    "    #here v is a vocabulary of n-grams, so will be the count of the ngrams\n",
    "    \n",
    "    return ret\n",
    "\n",
    "        \n",
    "def perplexity(model, onelessmodel, text, n = 4):\n",
    "    #this takes in text, which the existing probabilities and counts are used to \n",
    "    #come up with a number, all of the probabilities multiplied together. We probably can use log for this?\n",
    "    #Perplexity is a measure of how probable the model is at generating a sentence\n",
    "    #\n",
    "    #Perplexity is an integer - lower better\n",
    "    pp = 1\n",
    "    V = len(model)\n",
    "    for i, word in enumerate(text):\n",
    "        #calculate the probability of this word\n",
    "        #TODO implement log sum instead\n",
    "        pp *= 1/calculate_probability(model, onelessmodel, word, text[i-n:i-1])\n",
    "    return math.pow(pp, 1/V)\n",
    "\n",
    "def wikiperplexity(model, onelessmodel, text, n=4):\n",
    "    pp = 0\n",
    "    V = len(model)\n",
    "    for i, word in enumerate(text):\n",
    "        pp -= math.log(2, calculate_probability(model, onelessmodel, word, text[i-n:i-1]))\n",
    "    return math.pow(2, pp/V)\n",
    "#Currently, this works: \n",
    "#print(guten4[('the', 'children', 'of', 'israel')])\n",
    "\n",
    "print(calculate_probability(guten4, guten3, 'israel', ['the', 'children', 'of']))\n",
    "print(calculate_probability(guten4, guten3, 'were', ['children', 'of', 'israel']))\n",
    "print(wikiperplexity(guten4, guten3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))\n",
    "print(perplexity(guten4, guten3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))\n",
    "#perplexity(guten4, guten3, [\"the\", \"children\", \"of\", \"israel\", \"are\", \"well\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000033056111426\n",
      "1.0009467110114756\n"
     ]
    }
   ],
   "source": [
    "news = read_dir('news_data', 'news-corpus.txt')\n",
    "news = replace_unk(news, 5, 'news-unk.txt')\n",
    "news4 = ngram(4, news, 'news-4grams.txt')\n",
    "news3 = ngram(3, news, 'news-3grams.txt')\n",
    "\n",
    "print(wikiperplexity(news4, news3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))\n",
    "print(perplexity(news4, news3, ['the', 'children', 'of', 'israel', 'were', 'off', 'on', 'the', 'beaches', 'when']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging HMM\n",
    "\n",
    "First find the tag unigram and tag bigram counts from the corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import random\n",
    "#read in the file/s?\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#nltk.download('brown')\n",
    "def read_brown(directory, cachefile):\n",
    "    try:\n",
    "        text = read(cachefile)\n",
    "    except(FileNotFoundError):\n",
    "        corpus = []\n",
    "        base = directory\n",
    "        for file in listdir(base):\n",
    "            #print(file)\n",
    "            for sent in open(base + \"/\" + file):\n",
    "                if sent == \"\\n\": continue\n",
    "                wordlist = []\n",
    "                for word in sent.strip().split(' '):\n",
    "                    #split the word and it's tag\n",
    "                    if word == '': \n",
    "                        continue\n",
    "                    wordlist.append(word.split('/'))\n",
    "                    \n",
    "                corpus += [['<s>', '<s>']] + wordlist + [['</s>', '</s>']]\n",
    "        text = corpus\n",
    "        save(text, cachefile)\n",
    "    finally:\n",
    "        return text\n",
    "brown = read_brown('brown', 'brown-cache.txt')\n",
    "\n",
    "#calculate the word-tag counts\n",
    "#lets do this in the same dictionary way we did earlier\n",
    "\n",
    "def wordtag(text, outfile):\n",
    "    pairs = {}\n",
    "    for word in text:\n",
    "        #ignore the sentence tags\n",
    "        if (word == ['<s>', '<s>'] or word == ['</s>', '</s>']): continue\n",
    "        tagpair = tuple(word)\n",
    "        if tagpair in pairs.keys():\n",
    "            pairs[tagpair] += 1\n",
    "        else:\n",
    "             pairs[tagpair] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        for word in sorted(pairs, key=pairs.get, reverse=True):\n",
    "            f.write(' '.join(word) + ' ' + str(pairs[word]) + '\\n')\n",
    "    return pairs\n",
    "\n",
    "def tagunigram(text, outfile):\n",
    "    '''This is literally a unigram of the tag, t_n. That is we \n",
    "    will not consider the word association and will instead just\n",
    "    consider the impact of the counts of tags themselves.'''\n",
    "    unigrams = {}\n",
    "    for word in text:\n",
    "        #ignore the sentence tags\n",
    "        if (word == ['<s>', '<s>'] or word == ['</s>', '</s>']): continue \n",
    "        tag = word[1]\n",
    "        if tag in unigrams.keys():\n",
    "            unigrams[tag] += 1\n",
    "        else:\n",
    "             unigrams[tag] = 1\n",
    "    #save a textual representation of the dict to file\n",
    "    \n",
    "    with open(outfile, 'w') as f:\n",
    "        #TODO there is a problem with the way this joins - it's assuming a tuple\n",
    "        for word in sorted(unigrams, key=unigrams.get, reverse=True):\n",
    "            f.write(' '.join(word) + ' ' + str(unigrams[word]) + '\\n')\n",
    "    return unigrams\n",
    "    \n",
    "def savecounts(d, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for token in sorted(d, key=d.get, reverse=True):\n",
    "            f.write(' '.join(token) + ' ' + str(d[token]) + '\\n')\n",
    "            \n",
    "def tagbigram(text, outfile):\n",
    "    '''Here we consider both t_n and t_n-1 and report the counts. \n",
    "    Again we do not stop to consider the effects of the word association'''\n",
    "    bigrams = {}\n",
    "    for i in range(len(text)):\n",
    "        #ignore the sentence tags\n",
    "        if (text[i] == ['<s>', '<s>'] or text[i] == ['</s>', '</s>']): continue\n",
    "        t = text[i][1]\n",
    "        t1 = text[i-1][1]\n",
    "        if (t, t1) in bigrams.keys():\n",
    "            bigrams[(t, t1)] += 1\n",
    "        else:\n",
    "             bigrams[(t, t1)] = 1\n",
    "    savecounts(bigrams, outfile)\n",
    "    return bigrams\n",
    "    #save a textual representation of the dict to file\n",
    "\n",
    "\n",
    "def transition(bigramtags, unigramtags):\n",
    "    probabilities = {}\n",
    "    for bigram in bigramtags.keys():\n",
    "        probabilities[bigram] = bigramtags[bigram]/unigramtags[bigram[0]]\n",
    "    savecounts(probabilities, 'brownmeta/transition-probabilities.txt')\n",
    "    return probabilities\n",
    "\n",
    "def emission(wordtags, unigramtags):\n",
    "    emissionprob = {}\n",
    "    for wordpair in wordtags.keys():\n",
    "        emissionprob[wordpair] = wordtags[wordpair]/unigramtags[wordpair[1]]\n",
    "    savecounts(emissionprob, 'brownmeta/emission-probabilities.txt')\n",
    "    return emissionprob\n",
    "\n",
    "\n",
    "        \n",
    "tags = wordtag(brown, 'brownmeta/brownwordtag.txt')\n",
    "unitags = tagunigram(brown, 'brownmeta/brownuni.txt')\n",
    "bitags = tagbigram(brown, 'brownmeta/brownbigrams.txt')\n",
    "\n",
    "#These are both saved to file\n",
    "transitionprobs = transition(bitags, unitags)\n",
    "emissionprobs = emission(tags, unitags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxprob: 79410481308774000\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The/at', 'cat/nn-nc', 'sat/vbd']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "class postagger:\n",
    "    \n",
    "    def default(self):\n",
    "        return 0.0001\n",
    "    \n",
    "    def __init__(self, wordtags, unigramprobabilities, bigramprobabilities):\n",
    "        #Emission probabilities\n",
    "        self.wt = defaultdict(self.default, wordtags)\n",
    "        \n",
    "        self.up = defaultdict(self.default, unigramprobabilities)\n",
    "        \n",
    "        #Transition probabilities\n",
    "        self.bp = defaultdict(self.default, bigramprobabilities)\n",
    "    \n",
    "    def nextword(self, dct):\n",
    "        #select a next item based on a random number which is weighted by the probabilities\n",
    "        \n",
    "        #sum all of the probabilities and normalize\n",
    "        tot = 0\n",
    "        for value in dct.keys():\n",
    "            tot += dct[value]\n",
    "        normalised = {}\n",
    "        index = random.random()\n",
    "        for pair in dct.keys():\n",
    "            index -= dct[pair] / tot\n",
    "            if index <= 0.0:\n",
    "                return pair\n",
    "        \n",
    "    def predictSentence(self):\n",
    "        '''Will generate a sentence, with associated tags. \n",
    "        Output will contain sentence and sentence probability in a dict'''\n",
    "        sent = []\n",
    "        humansent = []\n",
    "        priortag = '<s>'\n",
    "        sentp = 0\n",
    "        while(priortag != '</s>' and priortag != '.'):\n",
    "            subset = {}\n",
    "            for tags in self.bp.keys():\n",
    "                if priortag == tags[1]:\n",
    "                    subset[tags] = self.bp[tags]\n",
    "            selectedbigram = self.nextword(subset)\n",
    "            \n",
    "            #capture tag probability\n",
    "            sentp -= math.log(subset[selectedbigram], 2)\n",
    "            currenttag = selectedbigram[0]\n",
    "            \n",
    "            potentialwordtags = {}\n",
    "            \n",
    "            for wordtag in self.wt.keys():\n",
    "                if currenttag == wordtag[1]:\n",
    "                    potentialwordtags[wordtag] = self.wt[wordtag]\n",
    "            currentword = self.nextword(potentialwordtags)\n",
    "            #capture word probability\n",
    "            sentp -= math.log(potentialwordtags[currentword], 2)\n",
    "            \n",
    "            sent.append('/'.join(currentword))\n",
    "            #humansent.append(currentword[0])\n",
    "            priortag = currenttag\n",
    "        return({'sentence': sent, 'probability': math.pow(2, sentp)})\n",
    "    \n",
    "    def viterbi(self, sentence):\n",
    "        '''Takes a tokenised sentence and will then apply some tags to it.'''\n",
    "        #remember states are the wordtags\n",
    "        startp = {}\n",
    "        for tags in self.bp.keys():\n",
    "                if '<s>' == tags[1]:\n",
    "                    startp[tags[0]] = self.bp[tags]\n",
    "        \n",
    "        viterbim = [{}]\n",
    "        #we will keep track of the backpointers using a list of dicts, with probabilities and previous recorded\n",
    "        #this will make the backtracing easy \n",
    "        \n",
    "        #Essentially the up.tags gives us a list of all of the POS tags\n",
    "        for state in startp.keys():\n",
    "            #Create the first column of the viterbi\n",
    "            \n",
    "            #TODO implement lambda smoothing. It will require changing how the probabilities are calculated\n",
    "            #and will require a return here for the unknowns which is calculated in place when needed\n",
    "\n",
    "            viterbim[0][state] = {'prev': None, 'probability': startp[state]*self.wt[(sentence[0], state)]}\n",
    "        \n",
    "        for i in range(1, len(sentence)):\n",
    "            #Find the maximum transition probability from the previous state to the current potential state\n",
    "            viterbim.append({})\n",
    "            for state in self.up.keys():\n",
    "                viterbim[i][state] = {'probability': 0, 'prev': None}\n",
    "                listofstateprobs = []\n",
    "                for prevstate in viterbim[i-1].keys():\n",
    "                    #TODO Lambda smoothing\n",
    "                    currentprob = viterbim[i-1][prevstate]['probability']*self.bp[(prevstate, state)]\n",
    "                    currentmax = viterbim[i][state]['probability']\n",
    "                    if (currentprob > currentmax):\n",
    "                        #This is basically saying, set the probability to the probability that this tag follows the previous tag\n",
    "                        #and multiply (by markov assumption) the emission probability of the word given that state\n",
    "                        \n",
    "                        viterbim[i][state] = {'probability': currentprob*self.wt[(sentence[i], state)], 'prev':prevstate}\n",
    "        #now find the highest probability state\n",
    "        taggedsentence = []\n",
    "        maxprob = max(prob['probability'] for prob in viterbim[-1].values())\n",
    "        #backtrack on this state\n",
    "        prevstate = None\n",
    "        \n",
    "        #iterate through backwards through viterbim\n",
    "        \n",
    "        for state, prob in viterbim[i].items():\n",
    "                if prob['probability'] == maxprob:\n",
    "                    prevstate = prob['prev']\n",
    "                    taggedsentence.append('/'.join([sentence[i], state]))\n",
    "                    \n",
    "        for i in range(len(viterbim)-1, 0, -1):\n",
    "            prevstate = viterbim[i][prevstate]['prev']\n",
    "            taggedsentence.insert(0, '/'.join([sentence[i-1], prevstate]))\n",
    "        return taggedsentence\n",
    "                \n",
    "\n",
    "pos = postagger(tags, unitags, bitags)\n",
    "sents = []\n",
    "with open('brownmeta/generatedsentences.txt', 'w') as f:\n",
    "    for i in range(5):\n",
    "        sents.append(pos.predictSentence())\n",
    "        f.write('{} Probability: {}\\n'.format(' '.join(sents[i]['sentence']), sents[i]['probability']))\n",
    "        \n",
    "with open('brownmeta/human-readablesentences.txt', 'w') as f:\n",
    "    for sent in sents:\n",
    "        f.write(' '.join([word.split('/')[0] for word in sent['sentence']]) + '\\n')    \n",
    "        \n",
    "pos.viterbi(['The', 'cat', 'sat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxprob: 2.0080252384283956e+42\n",
      "8\n",
      "maxprob: 3.1477207761221576e+97\n",
      "20\n",
      "maxprob: 3.021324982380108e+109\n",
      "24\n",
      "maxprob: 2.1955339299622646e+239\n",
      "53\n",
      "maxprob: 3.7176547643644863e+59\n",
      "13\n",
      "maxprob: 3.365011980748336e+52\n",
      "12\n",
      "maxprob: 9.942807467315857e+33\n",
      "10\n",
      "maxprob: 1.5800078389366676e+245\n",
      "58\n",
      "maxprob: 7.275540136710308e+34\n",
      "9\n",
      "maxprob: 4.654784038470841e+39\n",
      "9\n",
      "maxprob: 3.310088804696931e+100\n",
      "25\n",
      "maxprob: 6.820221933502801e+34\n",
      "9\n",
      "maxprob: 1.3794921079771193e+108\n",
      "23\n",
      "maxprob: 2.740226758248556e+25\n",
      "8\n",
      "maxprob: 2.3085513786579857e+85\n",
      "19\n",
      "maxprob: 7.977017574726705e+25\n",
      "7\n",
      "maxprob: 3.183289012453947e+49\n",
      "13\n",
      "maxprob: 1.5337830574162862e+38\n",
      "9\n",
      "maxprob: 1.8049563962042477e+107\n",
      "20\n",
      "maxprob: 3.748864718831112e+110\n",
      "23\n",
      "maxprob: 8.428131337233712e+59\n",
      "12\n",
      "maxprob: 1.3450249446538133e+109\n",
      "25\n",
      "maxprob: 4.28122840840757e+201\n",
      "44\n",
      "maxprob: 6.49513912383942e+61\n",
      "15\n",
      "maxprob: 1.5928714459221863e+107\n",
      "19\n",
      "maxprob: 5.729089095576724e+143\n",
      "30\n",
      "maxprob: 1.510424638859083e+67\n",
      "16\n",
      "maxprob: 1.8316315259107136e+31\n",
      "7\n",
      "maxprob: 3.191219054616047e+51\n",
      "11\n",
      "maxprob: 2.26263753523917e+145\n",
      "28\n",
      "maxprob: 2.960978726747843e+57\n",
      "15\n",
      "maxprob: 7.746246496695334e+46\n",
      "12\n",
      "maxprob: 2.3341782509392293e+113\n",
      "23\n",
      "maxprob: 3.713256797713509e+43\n",
      "11\n",
      "maxprob: 5.519938228928071e+54\n",
      "10\n",
      "maxprob: 1.7046322881813378e+16\n",
      "5\n",
      "maxprob: 3.6537752263632495e+43\n",
      "10\n",
      "maxprob: 3.954091780817073e+44\n",
      "10\n",
      "maxprob: 4.31954834752815e+84\n",
      "17\n",
      "maxprob: 2.8280124839255855e+133\n",
      "29\n",
      "maxprob: 5.3407547374234453e+154\n",
      "31\n",
      "maxprob: 3.380948357661286e+107\n",
      "21\n",
      "maxprob: 3.581773588317232e+44\n",
      "10\n",
      "maxprob: 1.305793203418419e+116\n",
      "26\n",
      "maxprob: 1.3771973742082674e+36\n",
      "8\n",
      "maxprob: 1.4775691065458468e+28\n",
      "8\n",
      "maxprob: 9.440871938310182e+110\n",
      "24\n",
      "maxprob: 5.617373723191542e+144\n",
      "28\n",
      "maxprob: 7.741484732284823e+93\n",
      "19\n",
      "maxprob: 4.001593671241936e+107\n",
      "24\n",
      "maxprob: 1.7360063135753248e+89\n",
      "22\n",
      "maxprob: 212661603.8753472\n",
      "4\n",
      "maxprob: 4.931295792322579e+90\n",
      "23\n",
      "maxprob: 1.6368997960539327e+97\n",
      "21\n",
      "maxprob: 1.7204832369954207e+129\n",
      "27\n",
      "maxprob: 6.2979961160194436e+134\n",
      "28\n",
      "maxprob: 2.8544055782657124e+126\n",
      "26\n",
      "maxprob: 9.823541967650845e+136\n",
      "28\n",
      "maxprob: 5.697256678785799e+193\n",
      "44\n",
      "maxprob: 1.0446060398764055e+131\n",
      "26\n",
      "maxprob: 2.175320936354649e+160\n",
      "36\n",
      "maxprob: 7.695136981167532e+120\n",
      "27\n",
      "maxprob: 9.921333325221339e+104\n",
      "22\n",
      "maxprob: 3.0017683212328973e+61\n",
      "12\n",
      "maxprob: 1.0838893083876078e+161\n",
      "34\n",
      "maxprob: 2.867499351537756e+121\n",
      "27\n",
      "maxprob: 3.2040299577198434e+130\n",
      "32\n",
      "maxprob: 8.954832599396884e+79\n",
      "18\n",
      "maxprob: 4.336372238683612e+52\n",
      "11\n",
      "maxprob: 3.251500483195947e+179\n",
      "38\n",
      "maxprob: 5.674241268504574e+119\n",
      "24\n",
      "maxprob: 1.775344888957056e+53\n",
      "14\n",
      "maxprob: 5.502641123237151e+152\n",
      "31\n",
      "maxprob: 1.883915522329825e+51\n",
      "12\n",
      "maxprob: 2.4678066890219872e+94\n",
      "23\n",
      "maxprob: 3.0105336171173067e+128\n",
      "27\n",
      "maxprob: 3.701778982837172e+125\n",
      "25\n",
      "maxprob: 1.0976713275912227e+74\n",
      "19\n",
      "maxprob: 1604918.259\n",
      "2\n",
      "maxprob: 8.975327625602733e+29\n",
      "9\n",
      "maxprob: 5.26543346877114e+44\n",
      "10\n",
      "maxprob: 1.574792699541144e+55\n",
      "14\n",
      "maxprob: 1107886.8360000001\n",
      "2\n",
      "maxprob: 8.02678948239168e+116\n",
      "26\n",
      "maxprob: 2.51938296116893e+68\n",
      "13\n",
      "maxprob: 2.326874629022499e+55\n",
      "13\n",
      "maxprob: 2.5357280929575984e+61\n",
      "15\n",
      "maxprob: 1.599239186634182e+79\n",
      "17\n",
      "maxprob: 2.2500717676208848e+95\n",
      "23\n",
      "maxprob: 3.963044357286545e+33\n",
      "8\n",
      "maxprob: 3.171678480247068e+77\n",
      "16\n",
      "maxprob: 9.444038584823708e+38\n",
      "9\n",
      "maxprob: 2.1360312867866232e+60\n",
      "13\n",
      "maxprob: 4.487991509940431e+149\n",
      "30\n",
      "maxprob: 3.8800517533478456e+101\n",
      "23\n",
      "maxprob: 9.148845091120974e+54\n",
      "11\n",
      "maxprob: 2.72577046913966e+90\n",
      "22\n",
      "maxprob: 3.164706296162717e+129\n",
      "26\n",
      "maxprob: 4.2589190093272573e+155\n",
      "33\n",
      "maxprob: 9.234051881128154e+103\n",
      "25\n",
      "maxprob: 1.039126474489869e+66\n",
      "15\n",
      "maxprob: 4.772271343840981e+71\n",
      "17\n",
      "maxprob: 1.897406793038261e+46\n",
      "13\n",
      "maxprob: 3.2126857694855814e+51\n",
      "11\n",
      "maxprob: 2.3967301677703837e+101\n",
      "21\n",
      "maxprob: 5.1337339554480445e+72\n",
      "19\n",
      "maxprob: 1.5444110521454043e+91\n",
      "18\n",
      "maxprob: 2.865100930704463e+60\n",
      "12\n",
      "maxprob: 1.1409053959390534e+96\n",
      "22\n",
      "maxprob: 3.068452702552526e+43\n",
      "10\n",
      "maxprob: 4.067703096288416e+70\n",
      "13\n",
      "maxprob: 4.84859722293412e+23\n",
      "6\n",
      "maxprob: 3.013171453407726e+99\n",
      "22\n",
      "maxprob: 4.364355347705655e+93\n",
      "22\n",
      "maxprob: 1.1893513596223013e+49\n",
      "10\n",
      "maxprob: 1.1676331294179756e+142\n",
      "30\n",
      "maxprob: 2.0451236070539533e+100\n",
      "27\n",
      "maxprob: 6.715470073878611e+20\n",
      "5\n",
      "maxprob: 5.404497491073148e+114\n",
      "24\n",
      "maxprob: 1.0974788953983454e+139\n",
      "29\n",
      "maxprob: 1.170530852206504e+69\n",
      "15\n",
      "maxprob: 1.6357992787059615e+145\n",
      "33\n",
      "maxprob: 1.315581953515767e+89\n",
      "21\n",
      "maxprob: 1.2413454568060485e+48\n",
      "10\n",
      "maxprob: 1.2996764365558676e+132\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "with open('brownmeta/science_sample.txt') as f:\n",
    "    sentencecounter = 0\n",
    "    sentence = False\n",
    "    \n",
    "    words = [[]]\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            if re.match('<.*>', line):\n",
    "                if sentence:\n",
    "                    words.append([])\n",
    "                    sentencecounter += 1\n",
    "                sentence = not sentence\n",
    "            else:\n",
    "                words[sentencecounter].append(line.strip())\n",
    "tags = []\n",
    "for sentence in words:\n",
    "    try:\n",
    "        tags.append(pos.viterbi(sentence))\n",
    "    except:\n",
    "        tags.append(['error'])\n",
    "with open('brownmeta/science-tagged.txt', 'w') as f:\n",
    "    for sent in tags:\n",
    "        f.write(' '.join(sent) + '\\n')   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of Viterbi HMM POS Tagger\n",
    "\n",
    "It is acknowledged that the smoothing function employed in this implementation of the veterbi algorithm is incorrect, and simply adds a constant to all unknown values. This will have a definite negative impact on performance, where unknown bigrams will have higher probability than infrequent but known bigrams. \n",
    "\n",
    "Look in the folder brownmeta/science-tagged.txt for the sentences and their associated tags."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
